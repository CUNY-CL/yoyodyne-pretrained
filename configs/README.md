The files in this directory are example YAML configuration files showing
Yoyodyne's use with various pre-trained encoders.

Since the model builds its own cross-attention, it is possible to mix and match
encoders and decoders.

Pre-trained encoders:

-   [mBERT
    (`google-bert/bert-base-multilingual-cased`)](https://huggingface.co/google-bert/bert-base-multilingual-cased)
-   [XLM-RoBERTa
    (`FacebookAI/xlm-roberta-base`)](https://huggingface.co/FacebookAI/xlm-roberta-base)
